# FSDP

# collective operations

refer to [nvidia collective ops](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html)

refer to [communciation blog 中文](https://adaning.github.io/posts/8982.html)

### 1️⃣ **Broadcast（Bcast）**
**作用**：一个进程（通常 rank 0）将相同的数据发送给所有其他进程。  
**使用场景**：
- 模型初始化时，将主进程的模型参数广播给所有工作进程。  
- 训练前同步配置或随机种子。


### 2️⃣ **Reduce**
**作用**：所有进程将各自的数据进行规约（如 sum、max、mean 等），结果只保存在一个目标进程（通常 rank 0）。  
**使用场景**：
- 计算全局指标，如各 GPU 上 loss 的平均值，只在主进程上输出。


### 3️⃣ **Allreduce**
**作用**：与 Reduce 类似，但规约结果会分发给所有进程（每个进程都得到相同的结果）。  
**使用场景**：
- **分布式训练中梯度同步（最常见用途）**。  
- 各 GPU 都需要全局平均梯度以保持模型一致。


### 4️⃣ **Gather**
**作用**：每个进程发送数据到一个目标进程，目标进程收集成一个列表。  
**使用场景**：
- 收集每个进程的结果到主节点，用于最终输出或评估。


### 5️⃣ **Allgather**
**作用**：每个进程将自己的数据广播给所有进程，所有人都得到完整集合。  
**使用场景**：
- **分布式推理或对齐阶段**，每个进程需要所有的分片结果。  
- 参数分片训练（如 ZeRO）中收集完整参数。


### 6️⃣ **Scatter**
**作用**：与 Gather 相反，一个进程将不同的数据块分发给不同的进程。  
**使用场景**：
- 任务分配或数据分片初始化阶段。


### 7️⃣ **ReduceScatter**
**作用**：先执行 Reduce（规约），然后将规约后的结果按块分散给各个进程。  
**使用场景**：
- **优化版 Allreduce**（节省带宽），在大规模分布式训练中常用。  
- Megatron-LM、DeepSpeed 等框架中用于高效梯度同步。


### 8️⃣ **Alltoall**
**作用**：每个进程将不同的数据块分别发送给每个其他进程（双向、全互换）。  
**使用场景**：
- 分布式矩阵乘法、MoE（Mixture of Experts）中专家负载重分配。  
- 通信密集型 shuffle 或分区操作。

具体的RS, AG都是ring的形式做的，即每个GPU把第$i$个分片传给下一个临近的GPU，同时接受上一个GPU的第$i-1$个分片。

传统 Allreduce(sum) = Reduce-Scatter(sum) + Allgather（数学上等价），后者允许：

1. 只在需要的地方拿到需要的那一块数据（分片/切块），

2. 与前/后向计算深度重叠，

3. 结合参数/优化器状态分片，从而减少不必要的数据回传与常驻内存，总体端到端更省带宽、更快、更省内存。

RS的通信量为$\frac{N-1}{N}G$，AG的通信量为$\frac{N-1}{N}G$，AR的通信量为$2\frac{N-1}{N}G$



# Parallelism



# ⚙️ DeepSpeed ZeRO 优化器原理与显存分析

在传统 **Data Parallel (DP)** 训练中，每个 GPU 都保存完整的：
- 模型参数（Parameters）
- 梯度（Gradients）
- 优化器状态（Optimizer States，如 Adam 的 `m`, `v`）

当模型规模大（数十亿参数）时，这些三份副本会造成巨大的显存浪费。

## 🚀 ZeRO (Zero Redundancy Optimizer) 的核心思想
**方法**：将参数、梯度、优化器状态在不同 GPU 之间进行分片（shard）。  

每个阶段逐步减少冗余：

| ZeRO 阶段 | 分片对象 | 仍冗余的部分 | 通信需求 | 主要收益 |
|------------|-----------|----------------|------------|-----------|
| **Stage 1** | 优化器状态 | 参数 + 梯度 | 低 | 优化器显存减少 ×N |
| **Stage 2** | 优化器状态 + 梯度 | 参数 | 中 | 梯度显存减少 ×N |
| **Stage 3** | 优化器状态 + 梯度 + 参数 | 无 | 高 | 参数显存也减少 ×N |

---

## ⚙️ 各阶段的机制详解

### 🟩 **ZeRO-1：分片优化器状态**
- **思路**：每个 rank 只保存优化器状态的一部分（例如 Adam 的 `m`, `v`）。
- **通信**：Allreduce 仍然用于同步完整梯度。
- **优点**：节省优化器状态内存（通常占总显存 2×参数量）。
- **缺点**：仍需在每个 GPU 上存完整参数与梯度。

---

### 🟦 **ZeRO-2：再分片梯度**
- **思路**：在反向传播后，对梯度进行 Reduce-Scatter，使每个 rank 只保留 1/N 的梯度分片。
- **通信**：用 **ReduceScatter** 代替 **Allreduce**，节省一半通信。
- **优点**：优化器状态 + 梯度都分片；显存进一步减少。
- **缺点**：参数仍全量保存（Forward 仍需全模型）。

---

### 🟥 **ZeRO-3：再分片参数**
- **思路**：参数也在 rank 间分片。  
  - 前向时，按需 Allgather 得到所需层参数；  
  - 反向时，对梯度做 ReduceScatter。
- **通信**：前向阶段按需 Allgather 参数，反向阶段 ReduceScatter 梯度。
- **优点**：参数 + 梯度 + 优化器状态全部分片。
- **缺点**：通信量最高（但可与计算重叠），需要精心调度。

---

ZeRO 是 “Data Parallel 形式的 Model Parallelism”

- **传统 MP**：模型按层或张量切分，每个 GPU 只保存一部分模型并负责相应计算。  
- **ZeRO**：虽然每个 GPU 最终也只持有模型的一个分片，但：
  - 数据并行的 **计算逻辑仍一致**（每张卡仍处理独立 batch 的样本），  
  - 只是把**参数/梯度/优化器状态的数据存储层面做了分片**。
  
因此，ZeRO 实质上是：
> **“在数据并行框架下实现的模型参数层面的分片化”**  
即：**Data Parallelism + Model Sharding** ⇒ “DP 形式的 Model Parallelism”。

---

## 📊 显存占用分析示例  
**假设**：训练一个 **7B 参数模型**，使用 **8×H100 (80 GB)**，Adam 优化器。

### 1️⃣ 基本假设
- 模型参数数量：7 × 10⁹  
- Adam 需要两份状态：`m` 和 `v`
- 每个参数在不同精度下的字节数：
  - FP16 / BF16：2 bytes  
  - FP32：4 bytes  
- 梯度通常与参数精度相同。

| 元素 | 占用 | 说明 |
|------|------|------|
| 参数 | 2 B | BF16 训练 |
| 梯度 | 2 B | 同上 |
| 优化器状态 (m+v) | 8 B | FP32 存储 |
| **合计（全量）** | **12 B/param** | DP 下每卡均完整保存 |


### 2️⃣ 传统 DP（无 ZeRO）
总显存需求：
$$
7\times10^9 \times 12\text{B} = 84\text{ GB}
$$
> 每张 H100 仅参数相关部分就占 ~84 GB，已超 80 GB（不可行）。


### 3️⃣ ZeRO 各阶段节省比例

| 阶段 | 优化器状态 | 梯度 | 参数 | 理论显存占用（相对 DP） | 约占显存 |
|------|--------------|--------|--------|----------------------------|-----------|
| **DP baseline** | 全量 | 全量 | 全量 | 1× | 84 GB |
| **ZeRO-1** | 1/8 | 全量 | 全量 | 0.375× | ~31.5 GB |
| **ZeRO-2** | 1/8 | 1/8 | 全量 | 0.265× | ~22 GB |
| **ZeRO-3** | 1/8 | 1/8 | 1/8 | 0.125× | ~10.5 GB |

> ZeRO-3 后，每卡显存降至 ~10 GB，用于参数 + 梯度 + 优化器状态，剩余显存足够放激活、缓存、通信 buffer 等。

### 4️⃣ 结论
- **ZeRO-1**：优化器状态分片 → 显存节省约 3×  
- **ZeRO-2**：再分梯度 → 显存节省约 4×  
- **ZeRO-3**：再分参数 → 显存节省约 8×  
- 7B 模型在 8×H100 上完全可行，甚至能支撑 30–40B 模型。


### ✅ 六、总结表

| ZeRO 阶段 | 分片内容 | 通信方式 | 主要优化 | 显存节省 | 典型框架 |
|------------|------------|-----------|-----------|------------|-------------|
| ZeRO-1 | 优化器状态 | Allreduce | 优化器分片 | ~3× | DeepSpeed |
| ZeRO-2 | + 梯度 | ReduceScatter | 梯度分片 | ~4× | DeepSpeed, Megatron-LM |
| ZeRO-3 | + 参数 | RS + Allgather | 参数分片（完全无冗余） | ~8× | DeepSpeed ZeRO-Offload/Infinity |

---

# ⚙️ 现代 LLM / MoE 训练与推理的数值精度策略总览（2025）

### 🧠 一、核心目标
现代大模型训练的精度策略需要同时满足三点：

1. **训练稳定性** —— 防止梯度爆炸、动量积累精度损失  
2. **显存效率** —— 尽量用低精度节省显存 / 通信带宽  
3. **收敛与泛化不受损** —— FP32 精度仅保留在必要环节  

---

### 🚀 二、训练阶段（Training Phase）

| 模块 | 常用精度 | 说明 | 举例 |
|------|-----------|------|------|
| **权重 (Weights)** | **BF16** 或 FP16 | 模型参数主副本，用于前向/反向 | Llama3: BF16；DeepSeek-R1: BF16 |
| **梯度 (Gradients)** | **BF16** 或 FP16 | 与权重同精度计算；再进行 allreduce | 减少通信与内存带宽 |
| **优化器状态 (Adam m, v)** | **FP32** | 长期累积需高精度，避免动量截断 | 所有框架：DeepSpeed, Megatron |
| **Master Weights** | **FP32**（可选） | 用于 FP32 更新后再 cast 回低精度 | 部分框架省略（BF16 稳定） |
| **激活 (Activations)** | **FP8 / BF16 / FP16**（可混合） | 存储开销大，常用低精度激活重计算 | Megatron-Core 使用 FP8 |
| **归一化统计 (RMSNorm / LayerNorm stats)** | **FP32** | 小张量但需高精度稳定性 | 防止数值震荡 |
| **通信（Allreduce / RS / AG）** | 与梯度精度相同（BF16/FP16） | NCCL 原生支持半精度通信 | DeepSpeed ZeRO, Megatron-LM |
| **损失标量 (loss)** | **FP32** | 计算与回传时用高精度累计 | 典型 AMP 实践 |

### 🔹 常见配置：
- **Llama3 / Qwen3 / DeepSeek-R1 / Mistral**：  
  - 主干：BF16  
  - 优化器状态：FP32  
  - 激活：FP8 或 BF16  
  - 归一化统计：FP32  
- **BF16 训练优于 FP16**：因 BF16 指数位多（8 位 vs FP16 的 5 位），不会溢出/下溢。

---

## 🧮 三、推理阶段（Inference Phase）

| 模块 | 常用精度 | 说明 | 举例 |
|------|-----------|------|------|
| **权重 (Weights)** | **FP8 / INT8 / INT4**（量化后） | 最常见节省显存的手段 | Qwen3-7B: INT4；DeepSeek-R1: FP8 |
| **激活 (Activations)** | **FP8 / FP16** | FP8-friendly 部署框架（如 TensorRT-LLM, DeepEP） |
| **KV Cache（注意力缓存）** | **FP16 / FP8 / INT8** | 可独立量化（影响速度与内存） |
| **归一化统计 / Softmax** | **FP32** | 小规模计算保持精度稳定 |
| **输出 Logits** | **FP16 / FP32** | 最终概率计算或采样阶段提升精度 |

---

## 🧩 四、MoE 特有的精度策略

| 部分 | 常用精度 | 说明 |
|------|-----------|------|
| **Router（门控网络）** | **FP32 或 BF16** | 确保专家选择稳定；低精度易抖动 |
| **专家 (Experts)** | **BF16 / FP8 / INT8** | 与主干一致；激活用更低精度 |
| **通信（AlltoAll）** | **BF16 / FP8** | 减少带宽；DeepSeek MoE 通信量化 |
| **辅助损失 (load balancing loss)** | **FP32** | 小张量，保持高精度 |

> 在 DeepSeek-R1、Qwen-MoE 等系统中：  
> - Router 前向通常用 FP32/BF16  
> - Experts 权重使用 FP8 (训练后量化)  
> - 通信采用混合精度（BF16 for reduce-scatter, FP8 for alltoall）

---

## 📊 五、典型系统配置一览

| 模型 / 框架 | 权重 | 梯度 | 优化器状态 | 激活 | 推理权重 | KV Cache |
|--------------|--------|----------|------------------|-----------|----------------|-------------|
| **Llama3** | BF16 | BF16 | FP32 | BF16 | INT8 / FP8 | FP16 |
| **Qwen3** | BF16 | BF16 | FP32 | FP8 / BF16 | INT4 / FP8 | FP8 |
| **DeepSeek-R1** | BF16 | BF16 | FP32 | FP8 | FP8 | FP8 |
| **Mistral 7B** | BF16 | BF16 | FP32 | BF16 | INT8 | FP16 |
| **Gemma 2B** | FP16 | FP16 | FP32 | FP16 | INT4 | FP8 |

---

## 🧠 六、总结

- **训练时**：  
  - 计算层：BF16 / FP16  
  - 优化器状态与归一化：FP32  
  - 激活：BF16 / FP8（重计算或低精度缓存）  

- **推理时**：  
  - 权重通常量化（INT8 / FP8 / INT4）  
  - 激活与 KV Cache 可用 FP8 / FP16  
  - Router / Softmax 等关键算子保 FP32 精度  

---



